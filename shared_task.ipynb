{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shared_task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarabic.araby as araby\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import re\n",
        "import torch.nn as nn\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import gc\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "S24hvx4reJdX"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/OSACT2022-sharedTask-train.txt',\n",
        "sep='\\t', names = ['index', 'tweet', 'offensive', 'hatespeech',\n",
        "                   'vulgar', 'violent'])\n",
        "dev = pd.read_csv('/content/OSACT2022-sharedTask-dev.txt',\n",
        "sep='\\t', names = ['index', 'tweet', 'offensive', 'hatespeech',\n",
        "                   'vulgar', 'violent'])"
      ],
      "metadata": {
        "id": "VaOozYg0fAnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('UBC-NLP/MARBERT')\n",
        "# marbert = AutoModel.from_pretrained('UBC-NLP/MARBERT', output_hidden_states= True)"
      ],
      "metadata": {
        "id": "OdErVZNLmMyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning, Tokenization, Encoding"
      ],
      "metadata": {
        "id": "fnbNqou-l2Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(tweet):\n",
        "    text = araby.strip_tashkeel(tweet)\n",
        "    text = araby.strip_tatweel(text)\n",
        "    clean_tweet = text.replace('@USER', '')\n",
        "    clean_tweet = clean_tweet.replace('URL', '')\n",
        "    clean_tweet=bytes(clean_tweet, 'utf-8').decode('utf-8','ignore')\n",
        "    clean_tweet = re.sub(r'[A-Z]+',' ', clean_tweet)\n",
        "    clean_tweet = clean_tweet.replace('>', '')\n",
        "    clean_tweet = clean_tweet.replace('<', '')\n",
        "    return clean_tweet"
      ],
      "metadata": {
        "id": "JASxFWJRiCvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_len(tweets, tokenizer= tokenizer):\n",
        "    max_len = 0\n",
        "    for tweet in tweets:\n",
        "        tokens = tokenizer.encode(tweet, add_special_tokens = True)\n",
        "        max_len = max(max_len, len(tokens))\n",
        "    return max_len"
      ],
      "metadata": {
        "id": "4xipnD6ZgNWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(tweets, max_len, tokenizer = tokenizer):\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    \n",
        "    for tweet in tweets:\n",
        "        encodings_dict = tokenizer.encode_plus(\n",
        "        tweet,\n",
        "        add_special_tokens = True,\n",
        "        max_length = max_len,\n",
        "        pad_to_max_length = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "        input_ids.append(encodings_dict['input_ids'])\n",
        "        attention_masks.append(encodings_dict['attention_mask'])\n",
        "\n",
        "  \n",
        "    input_tensor = torch.cat(input_ids, dim = 0)\n",
        "    attention_mask_tensor = torch.cat(attention_masks, dim = 0)\n",
        "    \n",
        "    return input_tensor, attention_mask_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "kv-G1dE3mfUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(input_ids, attention_masks,\n",
        "                       batch_size, labels = None):\n",
        "    \n",
        "    if(labels == None):\n",
        "      tensor_dataset = TensorDataset(input_ids, attention_masks)\n",
        "    else:\n",
        "      tensor_dataset = TensorDataset(input_ids, attention_masks, \n",
        "                                   labels)\n",
        "\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        tensor_dataset,\n",
        "        shuffle = False,\n",
        "        batch_size = batch_size\n",
        "    )\n",
        "    \n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "B4ABFsN4milt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {\n",
        "    'OFF':1,\n",
        "    'NOT_OFF':0\n",
        "}"
      ],
      "metadata": {
        "id": "5UW9Wl8sAhgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labels(hatespeech_labels, labels_dict = labels_dict):\n",
        "    labels = []\n",
        "    for label in hatespeech_labels:\n",
        "        \n",
        "        label_value = labels_dict[label]\n",
        "        labels.append(label_value)\n",
        "            \n",
        "    labels_tensor = torch.Tensor(labels)\n",
        "    labels_tensor = labels_tensor.long()\n",
        " \n",
        "    return labels_tensor"
      ],
      "metadata": {
        "id": "4NceqPHqAJhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.tweet.iloc[7394]\n",
        "train.drop(index = 7394, inplace = True)"
      ],
      "metadata": {
        "id": "QqoMTpbGKQ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['tweet'] = train['tweet'].apply(clean_data)\n",
        "dev['tweet'] = dev['tweet'].apply(clean_data)\n",
        "\n",
        "train_tweets = list(train['tweet'].values)\n",
        "train_max_len = get_max_len(train_tweets)\n",
        "\n",
        "dev_tweets = list(dev['tweet'].values)\n",
        "dev_max_len = get_max_len(dev_tweets)"
      ],
      "metadata": {
        "id": "n1TItWjlmlb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "off_labels_train = train.offensive.values\n",
        "off_labels_dev = dev.offensive.values\n",
        "\n",
        "labels_train = create_labels(off_labels_train)\n",
        "labels_dev = create_labels(off_labels_dev)"
      ],
      "metadata": {
        "id": "udEZe0PyASBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input, train_mask = encode(train_tweets, train_max_len)\n",
        "dev_input, dev_mask = encode(train_tweets, dev_max_len)\n"
      ],
      "metadata": {
        "id": "jiu6JvSenDQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = create_dataloaders(input_ids, attention_masks,\n",
        "                       labels = labels_train, batch_size=32)\n",
        "dev_dataloader = create_dataloaders(input_ids, attention_mask,\n",
        "                       labels = labels_dev, batch_size =32)"
      ],
      "metadata": {
        "id": "Ash3J97I8g5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training, Validation and Testing functions"
      ],
      "metadata": {
        "id": "ZdEh2YbX0h4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class lstm(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers,\n",
        "               classes):\n",
        "    super(lstm, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size,\n",
        "                        hidden_size,\n",
        "                        num_layers,\n",
        "                        batch_first = True,\n",
        "                        bidirectional = True)\n",
        "    \n",
        "    self.fc = nn.Linear(hidden_size *2, classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h0 = torch.zeros(self.num_layers*2,x.size(0), self.hidden_size).to(device)\n",
        "    c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "    output, _ = self.lstm(x, (h0, c0))\n",
        "    output = self.fc(output[:, 1, :])\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "kybNxwJ3pmPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm(input_size = 768,\n",
        "             hidden_size = train_max_len,\n",
        "             num_layers= 2,\n",
        "             classes =2)"
      ],
      "metadata": {
        "id": "7J5HzKAG5rtl"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-5\n",
        "num_epochs = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "_SsMM8oq3lO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f3c27a-ba9f-48ed-dc96-ca313ec3f371"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(dataloader):\n",
        "    preds = []\n",
        "    target = []\n",
        "\n",
        "    loop = tqdm(dataloader,leave = True)\n",
        "    accuracy = 0\n",
        "    len_labels = 0\n",
        "    for batch in loop:\n",
        "        embeddings = batch[0].to(device)\n",
        "        labels = batch[1]\n",
        "\n",
        "        scores = model(embeddings)\n",
        "\n",
        "        predictions= torch.argmax(scores.float(), dim=1).to('cpu').flatten()\n",
        "        labels = labels.flatten()\n",
        "        preds.extend(predictions)\n",
        "        target.extend(labels)\n",
        "    \n",
        "\n",
        "    f1 = f1_score(target, preds, average='macro')\n",
        "    acc = accuracy_score(target, preds)\n",
        "    recall = recall_score(target, preds)\n",
        "    precision = precision_score(target, preds)\n",
        "\n",
        "    results = {\n",
        "        'f1': f1,\n",
        "        'accuracy': acc,\n",
        "        'recall': recall,\n",
        "        'precision': precision,\n",
        "        'results': results\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "WT-PAoQH0al1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader,dev_dataloader, num_epochs, seed_val,\n",
        "          optimizer = optimizer, model = model, criterion = criterion):\n",
        "\n",
        "  seed_val = seed_val\n",
        "  \n",
        "  random.seed(seed_val)\n",
        "  seed = random.randint(1, seed_val)\n",
        "  torch.manual_seed(seed) \n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  validaition_results = []\n",
        "  best_f1 = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      loop = tqdm(train_dataloader, leave = True)\n",
        "      for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "        embeddings = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        scores = model(embeddings)\n",
        "        loss = criterion(scores, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "          \n",
        "\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "\n",
        "      validation_scores = validate(dev_dataloader)\n",
        "      best_f1 = max(best_f1, validation_scores['f1'])\n",
        "      print('f1 score for Epoch ' + str(epoch) + ' is: ' +  str(best_f1))\n",
        "      \n",
        "      \n",
        "  return validaition_results\n",
        "      \n"
      ],
      "metadata": {
        "id": "eKDo_X4R1F5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating Marbert's Isotropy"
      ],
      "metadata": {
        "id": "_YG6bsUXpKHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isotropy(representations):\n",
        "    eig_values, eig_vectors = np.linalg.eig(np.matmul(np.transpose(representations),\n",
        "                                                      representations))\n",
        "    max_f = -mt.inf\n",
        "    min_f =  mt.inf\n",
        "\n",
        "    for i in range(eig_vectors.shape[1]):\n",
        "        f = np.matmul(representations, np.expand_dims(eig_vectors[:, i], 1))\n",
        "        f = np.sum(np.exp(f))\n",
        "\n",
        "        min_f = min(min_f, f)\n",
        "        max_f = max(max_f, f)\n",
        "\n",
        "    isotropy = min_f / max_f\n",
        "\n",
        "    return isotropy"
      ],
      "metadata": {
        "id": "Gz2gd6FsjO4g"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(dataloader):\n",
        "\n",
        "  features = []\n",
        "\n",
        "  loop = tqdm(dataloader, leave = True)\n",
        "  for batch in loop:\n",
        "    with torch.no_grad():\n",
        "\n",
        "      input_ids = batch[0]\n",
        "      attention_mask_train = batch[1]\n",
        "      output = marbert(input_ids, attention_mask= attention_mask_train)[0]\n",
        "      \n",
        "      output = output.cpu().numpy().reshape((-1,768))\n",
        "      output = np.delete(output, [0, len(output)-1], axis= 0)\n",
        "\n",
        "      for feature in output:\n",
        "        features.append(feature)\n",
        "      break\n",
        "\n",
        "  return features, output"
      ],
      "metadata": {
        "id": "U28KtotojCar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = create_dataloaders(train_input, train_mask , batch_size = 1)\n",
        "train_features = extract_features(train_dataloader)"
      ],
      "metadata": {
        "id": "fKyUwfZ7jRq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_isotropy_value = isotropy(train_features)\n",
        "print(original_isotropy_value)"
      ],
      "metadata": {
        "id": "GiThxwlkjRnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Representations Isotropic"
      ],
      "metadata": {
        "id": "RlvgPILgkBMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_based(representations, n_cluster: int, n_pc: int):\n",
        "\n",
        "\n",
        "  centroid, label=clst.vq.kmeans2(representations, n_cluster, minit='points',\n",
        "                                  missing='warn', check_finite=True)\n",
        "  cluster_mean=[]\n",
        "  for i in range(max(label)+1):\n",
        "    sum=np.zeros([1,768]);\n",
        "    for j in np.nonzero(label == i)[0]:\n",
        "      sum=np.add(sum, representations[j])\n",
        "    cluster_mean.append(sum/len(label[label == i]))\n",
        "\n",
        "  zero_mean_representation=[]\n",
        "  for i in range(len(representations)):\n",
        "    zero_mean_representation.append((representations[i])-cluster_mean[label[i]])\n",
        "\n",
        "  cluster_representations={}\n",
        "  for i in range(n_cluster):\n",
        "    cluster_representations.update({i:{}})\n",
        "    for j in range(len(representations)):\n",
        "      if (label[j]==i):\n",
        "        cluster_representations[i].update({j:zero_mean_representation[j]})\n",
        "\n",
        "  cluster_representations2=[]\n",
        "  for j in range(n_cluster):\n",
        "    cluster_representations2.append([])\n",
        "    for key, value in cluster_representations[j].items():\n",
        "      cluster_representations2[j].append(value)\n",
        "\n",
        "  cluster_representations2=np.array(cluster_representations2)\n",
        "\n",
        "  model=PCA()\n",
        "  post_rep=np.zeros((representations.shape[0],representations.shape[1]))\n",
        "\n",
        "  for i in range(n_cluster):\n",
        "      model.fit(np.array(cluster_representations2[i]).reshape((-1,768)))\n",
        "      component = np.reshape(model.components_, (-1, 768))\n",
        "\n",
        "      for index in cluster_representations[i]:\n",
        "        sum_vec = np.zeros((1, 768))\n",
        "\n",
        "        for j in range(n_pc):\n",
        "                sum_vec = sum_vec + np.dot(cluster_representations[i][index],\n",
        "                          np.transpose(component)[:,j].reshape((768,1))) * component[j]\n",
        "        \n",
        "        post_rep[index]=cluster_representations[i][index] - sum_vec\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  return post_rep\n",
        "\n"
      ],
      "metadata": {
        "id": "rJ20ovdJpy11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}